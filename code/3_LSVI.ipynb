{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa0c5a8-776c-4295-b539-570bd521715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.state = 0\n",
    "        self.goal_state = size * size - 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Actions: 0: right, 1: down, 2: left, 3: up\n",
    "        x, y = self.state % self.size, self.state // self.size\n",
    "        \n",
    "        if action == 0:   # Right\n",
    "            x = min(x + 1, self.size - 1)\n",
    "        elif action == 1: # Down\n",
    "            y = min(y + 1, self.size - 1)\n",
    "        elif action == 2: # Left\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 3: # Up\n",
    "            y = max(y - 1, 0)\n",
    "            \n",
    "        new_state = y * self.size + x\n",
    "        reward = 1.0 if new_state == self.goal_state else 0.0\n",
    "        done = new_state == self.goal_state\n",
    "        \n",
    "        self.state = new_state\n",
    "        return new_state, reward, done\n",
    "\n",
    "def extract_features(state, action, size):\n",
    "    # One-hot encoding of state-action pairs\n",
    "    features = np.zeros(size * size * 4)\n",
    "    features[state * 4 + action] = 1\n",
    "    return features\n",
    "\n",
    "def least_squares_value_iteration(env, gamma=0.99, num_episodes=1000):\n",
    "    size = env.size\n",
    "    num_actions = 4\n",
    "    feature_size = size * size * num_actions\n",
    "    \n",
    "    theta = np.zeros(feature_size)\n",
    "    A = np.zeros((feature_size, feature_size))\n",
    "    b = np.zeros(feature_size)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action (epsilon-greedy)\n",
    "            if np.random.random() < 0.1:  # exploration\n",
    "                action = np.random.randint(num_actions)\n",
    "            else:  # exploitation\n",
    "                q_values = [np.dot(theta, extract_features(state, a, size)) for a in range(num_actions)]\n",
    "                action = np.argmax(q_values)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_features(state, action, size)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                next_q_values = [np.dot(theta, extract_features(next_state, a, size)) for a in range(num_actions)]\n",
    "                target = reward + gamma * max(next_q_values)\n",
    "            \n",
    "            # Update A and b matrices\n",
    "            A += np.outer(features, features)\n",
    "            b += target * features\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Solve for theta using least squares\n",
    "        if episode % 10 == 0:  # Update theta periodically\n",
    "            theta = np.linalg.solve(A + 0.1 * np.eye(feature_size), b)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "def test_policy(env, theta):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        q_values = [np.dot(theta, extract_features(state, a, env.size)) for a in range(4)]\n",
    "        action = np.argmax(q_values)\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8479460a-254c-4425-9d59-28f0854fdc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward with learned policy: 1.0\n",
      "\n",
      "Value function:\n",
      "0.36\t0.56\t0.65\t0.34\t0.27\t\n",
      "0.54\t0.73\t0.86\t0.80\t0.72\t\n",
      "0.62\t0.78\t0.94\t0.97\t0.98\t\n",
      "0.00\t0.64\t0.84\t0.99\t1.00\t\n",
      "0.00\t0.00\t0.89\t0.99\t0.00\t\n"
     ]
    }
   ],
   "source": [
    "# Create environment and run algorithm\n",
    "env = GridWorld(size=5)\n",
    "theta = least_squares_value_iteration(env)\n",
    "\n",
    "# Test the learned policy\n",
    "total_reward = test_policy(env, theta)\n",
    "print(f\"Total reward with learned policy: {total_reward}\")\n",
    "\n",
    "# Print value function\n",
    "print(\"\\nValue function:\")\n",
    "for y in range(env.size):\n",
    "    row = \"\"\n",
    "    for x in range(env.size):\n",
    "        state = y * env.size + x\n",
    "        value = max([np.dot(theta, extract_features(state, a, env.size)) for a in range(4)])\n",
    "        row += f\"{value:.2f}\\t\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9226f6c-0556-4c12-8ca9-589fde81d846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
